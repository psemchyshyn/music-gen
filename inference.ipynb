{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "from music21 import chord, note, stream, tempo, duration\n",
    "from lightning.data import MusicDataWrapper, MusicDataWrapperCNN\n",
    "from lightning.rnn import LitAttentionRNN\n",
    "from lightning.seq2seq import LitSeq2Seq\n",
    "from lightning.vae import LitVAE\n",
    "from lightning.seg import LitMusicSeg\n",
    "from lightning.gan import MuseGAN\n",
    "\n",
    "def show_and_write(notes, durs, fn):\n",
    "    streamm = stream.Stream()\n",
    "    for notee, dur in zip(notes, durs):\n",
    "        if notee == \"START\" or not dur:\n",
    "            continue\n",
    "\n",
    "        if '.' in notee:\n",
    "            element = chord.Chord(notee.split(\".\"))\n",
    "        else:\n",
    "            element = note.Note(notee)\n",
    "\n",
    "        streamm.append(element)\n",
    "\n",
    "    streamm.write(\"midi\", fn)\n",
    "\n",
    "def get_music_from_tokens(tokenizer_ds, token_notes, token_durs):\n",
    "    notes = durs = []\n",
    "    for tok_note, tok_dur in zip(token_notes, token_durs):\n",
    "        notes.append(tokenizer_ds.tokens_to_notes[tok_note.item()])\n",
    "        durs.append(tokenizer_ds.tokens_to_durations[tok_dur.item()])\n",
    "    return notes, durs\n",
    "\n",
    "\n",
    "def notes_to_midi(midi_note_score, filename = None):\n",
    "    parts = stream.Score()\n",
    "    parts.append(tempo.MetronomeMark(number= 66))\n",
    "\n",
    "    for i in range(4):\n",
    "        s = stream.Part()\n",
    "        dur = 1/4\n",
    "        for idx, x in enumerate(midi_note_score[:, i]):\n",
    "            if np.isnan(x):\n",
    "                n = note.Rest(dur)\n",
    "            else:\n",
    "                x = int(x)\n",
    "                n = note.Note(x)\n",
    "                n.duration = duration.Duration(dur)\n",
    "            s.append(n)\n",
    "\n",
    "        parts.append(s)\n",
    "\n",
    "    parts.write('midi', fp=filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"configs/config_rnn.yaml\", \"r\") as f:\n",
    "    config_rnn = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "checkpoint_path = \"checkpoints\\model-epoch=09-val_loss=2.29.ckpt\"\n",
    "dm_rnn = MusicDataWrapper(config_rnn)\n",
    "lit_rnn = LitAttentionRNN(config_rnn, dm_rnn.num_notes_classes, dm_rnn.num_duration_classes)\n",
    "lit_rnn_trained = LitAttentionRNN.load_from_checkpoint(checkpoint_path, config=config_rnn, input_note_size=dm_rnn.num_notes_classes, input_dur_size=dm_rnn.num_duration_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes, durs = lit_rnn.generate(dm_rnn.dataset)\n",
    "show_and_write(notes, durs, \"examples/untrained_rnn.midi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes, durs = lit_rnn_trained.generate(dm_rnn.dataset)\n",
    "show_and_write(notes, durs, \"examples/trained_rnn.midi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LitSeq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (note_emb): Embedding(471, 100)\n",
       "    (dur_emb): Embedding(18, 100)\n",
       "    (rnn): GRU(200, 256, num_layers=2, batch_first=True)\n",
       "    (fc_notes): Linear(in_features=256, out_features=471, bias=True)\n",
       "    (fc_durs): Linear(in_features=256, out_features=18, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (note_emb): Embedding(471, 100)\n",
       "    (dur_emb): Embedding(18, 100)\n",
       "    (rnn): GRU(200, 256, num_layers=2, batch_first=True)\n",
       "    (fc_notes): Linear(in_features=256, out_features=471, bias=True)\n",
       "    (fc_durs): Linear(in_features=256, out_features=18, bias=True)\n",
       "  )\n",
       "  (model): Seq2Seq(\n",
       "    (encoder): Encoder(\n",
       "      (note_emb): Embedding(471, 100)\n",
       "      (dur_emb): Embedding(18, 100)\n",
       "      (rnn): GRU(200, 256, num_layers=2, batch_first=True)\n",
       "      (fc_notes): Linear(in_features=256, out_features=471, bias=True)\n",
       "      (fc_durs): Linear(in_features=256, out_features=18, bias=True)\n",
       "    )\n",
       "    (decoder): Decoder(\n",
       "      (note_emb): Embedding(471, 100)\n",
       "      (dur_emb): Embedding(18, 100)\n",
       "      (rnn): GRU(200, 256, num_layers=2, batch_first=True)\n",
       "      (fc_notes): Linear(in_features=256, out_features=471, bias=True)\n",
       "      (fc_durs): Linear(in_features=256, out_features=18, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (train_notes_metrics): MetricCollection(\n",
       "    (Accuracy): Accuracy()\n",
       "    (Precision): Precision()\n",
       "    (Recall): Recall()\n",
       "    (F1Score): F1Score(),\n",
       "    prefix=train_notes_\n",
       "  )\n",
       "  (train_durs_metrics): MetricCollection(\n",
       "    (Accuracy): Accuracy()\n",
       "    (Precision): Precision()\n",
       "    (Recall): Recall()\n",
       "    (F1Score): F1Score(),\n",
       "    prefix=train_durs_\n",
       "  )\n",
       "  (val_notes_metrics): MetricCollection(\n",
       "    (Accuracy): Accuracy()\n",
       "    (Precision): Precision()\n",
       "    (Recall): Recall()\n",
       "    (F1Score): F1Score(),\n",
       "    prefix=val_notes_\n",
       "  )\n",
       "  (val_durs_metrics): MetricCollection(\n",
       "    (Accuracy): Accuracy()\n",
       "    (Precision): Precision()\n",
       "    (Recall): Recall()\n",
       "    (F1Score): F1Score(),\n",
       "    prefix=val_durs_\n",
       "  )\n",
       "  (test_notes_metrics): MetricCollection(\n",
       "    (Accuracy): Accuracy()\n",
       "    (Precision): Precision()\n",
       "    (Recall): Recall()\n",
       "    (F1Score): F1Score(),\n",
       "    prefix=test_notes_\n",
       "  )\n",
       "  (test_durs_metrics): MetricCollection(\n",
       "    (Accuracy): Accuracy()\n",
       "    (Precision): Precision()\n",
       "    (Recall): Recall()\n",
       "    (F1Score): F1Score(),\n",
       "    prefix=test_durs_\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"configs/config_seq2seq.yaml\", \"r\") as f:\n",
    "    config_seq2seq = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "checkpoint_path = \"checkpoints_seq2seq\\model-epoch=06-val_loss=100.53.ckpt\"\n",
    "dm_seq2seq = MusicDataWrapper(config_seq2seq)\n",
    "lit_seq2seq = LitSeq2Seq(config_seq2seq, dm_seq2seq.num_notes_classes, dm_seq2seq.num_duration_classes)\n",
    "lit_seq2seq_trained = LitSeq2Seq.load_from_checkpoint(checkpoint_path, config=config_seq2seq, input_note_size=dm_seq2seq.num_notes_classes, input_dur_size=dm_seq2seq.num_duration_classes)\n",
    "lit_seq2seq_trained.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes, durs = lit_seq2seq.generate(dm_seq2seq.dataset, start_seq=([\"START\"], [0]))\n",
    "show_and_write(notes, durs, \"examples/untrained_seq2seq.midi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes, durs = lit_seq2seq_trained.generate(dm_seq2seq.dataset, start_seq=([\"START\"]*15, [0]*15))\n",
    "show_and_write(notes, durs, \"examples/trained_seq2seq.midi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LitVAE(\n",
       "  (encoder): Encoder(\n",
       "    (enc_conv1): Conv2d(4, 64, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (enc_conv2): Conv2d(64, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (enc_conv3): Conv2d(128, 256, kernel_size=(5, 2), stride=(1, 1))\n",
       "    (enc_batch1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (enc_batch2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (enc_batch3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (enc_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (enc_mu): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (enc_sigma): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (dropout): Dropout(p=0.4, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (dec_lin): Linear(in_features=64, out_features=256, bias=True)\n",
       "    (dec_conv1): ConvTranspose2d(256, 128, kernel_size=(5, 2), stride=(1, 1))\n",
       "    (dec_conv2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(4, 4), output_padding=(1, 0))\n",
       "    (dec_conv3): ConvTranspose2d(64, 4, kernel_size=(3, 4), stride=(4, 4))\n",
       "    (dec_batch3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dec_batch2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dec_batch1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (model): ConvolutionalVAE(\n",
       "    (encoder): Encoder(\n",
       "      (enc_conv1): Conv2d(4, 64, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (enc_conv2): Conv2d(64, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (enc_conv3): Conv2d(128, 256, kernel_size=(5, 2), stride=(1, 1))\n",
       "      (enc_batch1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (enc_batch2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (enc_batch3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (enc_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (enc_mu): Linear(in_features=256, out_features=64, bias=True)\n",
       "      (enc_sigma): Linear(in_features=256, out_features=64, bias=True)\n",
       "      (dropout): Dropout(p=0.4, inplace=False)\n",
       "    )\n",
       "    (decoder): Decoder(\n",
       "      (dec_lin): Linear(in_features=64, out_features=256, bias=True)\n",
       "      (dec_conv1): ConvTranspose2d(256, 128, kernel_size=(5, 2), stride=(1, 1))\n",
       "      (dec_conv2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(4, 4), output_padding=(1, 0))\n",
       "      (dec_conv3): ConvTranspose2d(64, 4, kernel_size=(3, 4), stride=(4, 4))\n",
       "      (dec_batch3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (dec_batch2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (dec_batch1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"configs/config_vae.yaml\", \"r\") as f:\n",
    "    config_vae = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "checkpoint_path = \"checkpoints_vae\\model-epoch=29-valid_loss=714.52.ckpt\"\n",
    "dm_vae = MusicDataWrapperCNN(config_vae)\n",
    "lit_vae = LitVAE(config_vae)\n",
    "lit_vae_trained = LitVAE.load_from_checkpoint(checkpoint_path, config=config_vae)\n",
    "lit_vae.eval()\n",
    "lit_vae_trained.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = lit_vae.model.encoder.N.sample(torch.Size([32, config_vae[\"model\"][\"latent_size\"]]))\n",
    "rec = lit_vae.generate(z)\n",
    "notes_to_midi(rec[0], f\"examples/untrained_vae-mse.midi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = lit_vae_trained.model.encoder.N.sample(torch.Size([32, config_vae[\"model\"][\"latent_size\"]]))\n",
    "rec = lit_vae_trained.generate(z)\n",
    "notes_to_midi(rec[0], f\"examples/trained_vae-mse.midi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional VAE cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LitVAE(\n",
       "  (encoder): Encoder(\n",
       "    (enc_conv1): Conv2d(4, 64, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (enc_conv2): Conv2d(64, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (enc_conv3): Conv2d(128, 256, kernel_size=(5, 2), stride=(1, 1))\n",
       "    (enc_batch1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (enc_batch2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (enc_batch3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (enc_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (enc_mu): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (enc_sigma): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (dropout): Dropout(p=0.4, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (dec_lin): Linear(in_features=64, out_features=256, bias=True)\n",
       "    (dec_conv1): ConvTranspose2d(256, 128, kernel_size=(5, 2), stride=(1, 1))\n",
       "    (dec_conv2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(4, 4), output_padding=(1, 0))\n",
       "    (dec_conv3): ConvTranspose2d(64, 4, kernel_size=(3, 4), stride=(4, 4))\n",
       "    (dec_batch3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dec_batch2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dec_batch1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (model): ConvolutionalVAE(\n",
       "    (encoder): Encoder(\n",
       "      (enc_conv1): Conv2d(4, 64, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (enc_conv2): Conv2d(64, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (enc_conv3): Conv2d(128, 256, kernel_size=(5, 2), stride=(1, 1))\n",
       "      (enc_batch1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (enc_batch2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (enc_batch3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (enc_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (enc_mu): Linear(in_features=256, out_features=64, bias=True)\n",
       "      (enc_sigma): Linear(in_features=256, out_features=64, bias=True)\n",
       "      (dropout): Dropout(p=0.4, inplace=False)\n",
       "    )\n",
       "    (decoder): Decoder(\n",
       "      (dec_lin): Linear(in_features=64, out_features=256, bias=True)\n",
       "      (dec_conv1): ConvTranspose2d(256, 128, kernel_size=(5, 2), stride=(1, 1))\n",
       "      (dec_conv2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(4, 4), output_padding=(1, 0))\n",
       "      (dec_conv3): ConvTranspose2d(64, 4, kernel_size=(3, 4), stride=(4, 4))\n",
       "      (dec_batch3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (dec_batch2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (dec_batch1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"configs/config_vae.yaml\", \"r\") as f:\n",
    "    config_vae = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "checkpoint_path = \"checkpoints_vae_cross_entropy\\model-epoch=29-valid_loss=718.03.ckpt\"\n",
    "dm_vae = MusicDataWrapperCNN(config_vae)\n",
    "lit_vae = LitVAE(config_vae)\n",
    "lit_vae_trained = LitVAE.load_from_checkpoint(checkpoint_path, config=config_vae)\n",
    "lit_vae.eval()\n",
    "lit_vae_trained.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = lit_vae.model.encoder.N.sample(torch.Size([32, config_vae[\"model\"][\"latent_size\"]]))\n",
    "rec = lit_vae.generate(z)\n",
    "notes_to_midi(rec[0], f\"examples/untrained_vae-cross-entropy.midi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = lit_vae_trained.model.encoder.N.sample(torch.Size([32, config_vae[\"model\"][\"latent_size\"]]))\n",
    "rec = lit_vae_trained.generate(z)\n",
    "notes_to_midi(rec[0], f\"examples/trained_vae-cross-entropy.midi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional U-Net like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LitMusicSeg(\n",
       "  (model): MusicSeg(\n",
       "    (enc_conv1): Conv2d(4, 64, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (enc_conv2): Conv2d(64, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (enc_conv3): Conv2d(128, 256, kernel_size=(5, 2), stride=(1, 1))\n",
       "    (enc_batch1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (enc_batch2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (enc_batch3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout): Dropout(p=0.4, inplace=False)\n",
       "    (dec_conv1): ConvTranspose2d(256, 128, kernel_size=(5, 2), stride=(1, 1))\n",
       "    (dec_conv2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(4, 4), output_padding=(1, 0))\n",
       "    (dec_conv3): ConvTranspose2d(64, 4, kernel_size=(3, 4), stride=(4, 4))\n",
       "    (dec_batch3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dec_batch2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"configs/config_seg.yaml\", \"r\") as f:\n",
    "    config_seg = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "checkpoint_path = \"checkpoints_seg_mse\\model-epoch=14-valid_loss=0.01-v1.ckpt\"\n",
    "dm_seg = MusicDataWrapperCNN(config_seg)\n",
    "lit_seg = LitMusicSeg(config_seg)\n",
    "lit_seg_trained = LitMusicSeg.load_from_checkpoint(checkpoint_path, config=config_seg)\n",
    "lit_seg.eval()\n",
    "lit_seg_trained.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(dm_seg.val_dataloader()))\n",
    "gen = lit_seg.generate(data[0])\n",
    "notes_to_midi(gen[0], \"examples/untrained_seg_mse.midi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(dm_seg.val_dataloader()))\n",
    "x, y = data\n",
    "gen = lit_seg_trained.generate(x)\n",
    "y = torch.argmax(y.permute(0, 3, 1, 2), dim=3)\n",
    "x = torch.argmax(x.permute(0, 3, 1, 2), dim=3)\n",
    "\n",
    "notes_to_midi(gen[0], \"examples/seg_mse/pred.midi\")\n",
    "notes_to_midi(y[0].numpy(), \"examples/seg_mse/orig.midi\")\n",
    "notes_to_midi(x[0].numpy(), \"examples/seg_mse/start.midi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Unet-like cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LitMusicSeg(\n",
       "  (model): MusicSeg(\n",
       "    (enc_conv1): Conv2d(4, 64, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (enc_conv2): Conv2d(64, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (enc_conv3): Conv2d(128, 256, kernel_size=(5, 2), stride=(1, 1))\n",
       "    (enc_batch1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (enc_batch2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (enc_batch3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout): Dropout(p=0.4, inplace=False)\n",
       "    (dec_conv1): ConvTranspose2d(256, 128, kernel_size=(5, 2), stride=(1, 1))\n",
       "    (dec_conv2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(4, 4), output_padding=(1, 0))\n",
       "    (dec_conv3): ConvTranspose2d(64, 4, kernel_size=(3, 4), stride=(4, 4))\n",
       "    (dec_batch3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dec_batch2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"configs/config_seg.yaml\", \"r\") as f:\n",
    "    config_seg = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "checkpoint_path = \"checkpoints_seg_cross-entropy\\model-epoch=05-valid_loss=3.43.ckpt\"\n",
    "dm_seg = MusicDataWrapperCNN(config_seg)\n",
    "lit_seg = LitMusicSeg(config_seg)\n",
    "lit_seg_trained = LitMusicSeg.load_from_checkpoint(checkpoint_path, config=config_seg)\n",
    "lit_seg.eval()\n",
    "lit_seg_trained.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(dm_seg.val_dataloader()))\n",
    "gen = lit_seg.generate(data[0])\n",
    "notes_to_midi(gen[0], \"examples/untrained_seg_cross_entropy.midi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(dm_seg.val_dataloader()))\n",
    "x, y = data\n",
    "gen = lit_seg_trained.generate(x)\n",
    "y = torch.argmax(y.permute(0, 3, 1, 2), dim=3)\n",
    "x = torch.argmax(x.permute(0, 3, 1, 2), dim=3)\n",
    "\n",
    "notes_to_midi(gen[0], f\"examples/seg_cross_entropy/pred.midi\")\n",
    "notes_to_midi(y[0].numpy(), f\"examples/seg_cross_entropy/orig.midi\")\n",
    "notes_to_midi(x[0].numpy(), f\"examples/seg_cross_entropy/start.midi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MuseGAN(\n",
       "  (discriminator): Discriminator(\n",
       "    (conv1): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv2): Conv2d(64, 128, kernel_size=(12, 1), stride=(12, 1), padding=(6, 0))\n",
       "    (conv3): Conv2d(128, 128, kernel_size=(7, 1), stride=(7, 1), padding=(3, 0))\n",
       "    (conv4): Conv2d(128, 128, kernel_size=(1, 2), stride=(1, 2), padding=(0, 1))\n",
       "    (conv5): Conv2d(128, 128, kernel_size=(1, 2), stride=(1, 2), padding=(0, 1))\n",
       "    (conv6): Conv2d(128, 256, kernel_size=(1, 4), stride=(1, 2), padding=(0, 1))\n",
       "    (conv7): Conv2d(256, 512, kernel_size=(1, 3), stride=(1, 2))\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (dense): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (output): Linear(in_features=1024, out_features=1, bias=True)\n",
       "  )\n",
       "  (generator): Generator(\n",
       "    (unflatten): Unflatten(dim=1, unflattened_size=(512, 1, 1))\n",
       "    (deconv1): ConvTranspose2d(512, 512, kernel_size=(1, 3), stride=(1, 2))\n",
       "    (deconv2): ConvTranspose2d(512, 256, kernel_size=(1, 4), stride=(1, 2))\n",
       "    (deconv3): ConvTranspose2d(256, 128, kernel_size=(1, 2), stride=(1, 2))\n",
       "    (deconv4): ConvTranspose2d(128, 128, kernel_size=(1, 2), stride=(1, 2))\n",
       "    (deconv5): ConvTranspose2d(128, 128, kernel_size=(7, 1), stride=(7, 1))\n",
       "    (deconv6): ConvTranspose2d(128, 128, kernel_size=(12, 1), stride=(12, 1), padding=(1, 0))\n",
       "    (deconv7): ConvTranspose2d(128, 64, kernel_size=(2, 1), stride=(1, 1))\n",
       "    (deconv8): ConvTranspose2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (batch_n1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (batch_n2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (batch_n3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (batch_n4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (batch_n5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (batch_n6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (batch_n7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"configs/config_gan.yaml\", \"r\") as f:\n",
    "    config_gan = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "checkpoint_path = \"checkpoints_gans\\model-epoch=299-train_disc_loss=-168.32.ckpt\"\n",
    "dm_seg = MusicDataWrapperCNN(config_gan)\n",
    "lit_gan = MuseGAN(config_gan)\n",
    "lit_gan_trained = MuseGAN.load_from_checkpoint(checkpoint_path, config=config_gan)\n",
    "lit_gan.eval()\n",
    "lit_gan_trained.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msemc\\miniconda3\\envs\\music-gen\\lib\\site-packages\\torch\\nn\\functional.py:1933: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "z = torch.randn(12, config_gan[\"model\"][\"latent_size\"])\n",
    "generated = lit_gan.generate(z)\n",
    "notes_to_midi(generated[0], f\"examples/untrained_gan.midi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msemc\\miniconda3\\envs\\music-gen\\lib\\site-packages\\torch\\nn\\functional.py:1933: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "z = torch.randn(12, config_gan[\"model\"][\"latent_size\"])\n",
    "generated = lit_gan_trained.generate(z)\n",
    "for i in range(12):\n",
    "    notes_to_midi(generated[i], f\"examples/gan299/trained_gan{i}.midi\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9f91a12e41d9525743075a4f29f964af19eccf88374fc768e771b849eb39165b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('music-gen')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
