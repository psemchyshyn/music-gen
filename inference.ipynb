{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import chord, note, stream\n",
    "import yaml\n",
    "from lightning.data import MusicDataWrapper\n",
    "from lightning.rnn import LitAttentionRNN\n",
    "from lightning.seq2seq import LitSeq2Seq\n",
    "\n",
    "def show_and_write(notes, durs, fn):\n",
    "    streamm = stream.Stream()\n",
    "    for notee, dur in zip(notes, durs):\n",
    "        if notee == \"START\" or not dur:\n",
    "            continue\n",
    "\n",
    "        if '.' in notee:\n",
    "            element = chord.Chord(notee.split(\".\"))\n",
    "        else:\n",
    "            element = note.Note(notee)\n",
    "\n",
    "        streamm.append(element)\n",
    "\n",
    "    streamm.write(\"midi\", fn)\n",
    "\n",
    "def get_music_from_tokens(tokenizer_ds, token_notes, token_durs):\n",
    "    for tok_note, tok_dur in zip(token_notes, token_durs):\n",
    "        notes.append(tokenizer_ds.tokens_to_notes[tok_note.item()])\n",
    "        durs.append(tokenizer_ds.tokens_to_durations[tok_dur.item()])\n",
    "    return notes, durs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config_rnn.yaml\", \"r\") as f:\n",
    "    config_rnn = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "checkpoint_path = \"checkpoints\\model-epoch=09-val_loss=2.29.ckpt\"\n",
    "dm_rnn = MusicDataWrapper(config_rnn)\n",
    "lit_rnn = LitAttentionRNN(config_rnn, dm_rnn.num_notes_classes, dm_rnn.num_duration_classes)\n",
    "lit_rnn_trained = LitAttentionRNN.load_from_checkpoint(checkpoint_path, config=config_rnn, input_note_size=dm_rnn.num_notes_classes, input_dur_size=dm_rnn.num_duration_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes, durs = lit_rnn.generate(dm_rnn.dataset)\n",
    "show_and_write(notes, durs, \"untrained_rnn.midi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes, durs = lit_rnn_trained.generate(dm_rnn.dataset)\n",
    "show_and_write(notes, durs, \"trained_rnn.midi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config_seq2seq.yaml\", \"r\") as f:\n",
    "    config_seq2seq = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "checkpoint_path = \"checkpoints_seq2seq\\model-epoch=06-val_loss=100.53.ckpt\"\n",
    "dm_seq2seq = MusicDataWrapper(config_seq2seq)\n",
    "lit_seq2seq = LitSeq2Seq(config_seq2seq, dm_seq2seq.num_notes_classes, dm_seq2seq.num_duration_classes)\n",
    "lit_seq2seq_trained = LitSeq2Seq.load_from_checkpoint(checkpoint_path, config=config_seq2seq, input_note_size=dm_seq2seq.num_notes_classes, input_dur_size=dm_seq2seq.num_duration_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes, durs = lit_seq2seq.generate(dm_seq2seq.dataset, start_seq=([\"START\"], [0]))\n",
    "show_and_write(notes, durs, \"untrained_seq2seq.midi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes, durs = lit_seq2seq_trained.generate(dm_seq2seq.dataset, start_seq=([\"A4\", \"A4\"], [1, 1]))\n",
    "show_and_write(notes, durs, \"trained_seq2seq.midi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = dm_seq2seq.dataset\n",
    "position = 10000\n",
    "show_and_write(*get_music_from_tokens(tokenizer, *tokenizer[5000]), \"example.midi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = 10000\n",
    "notes, durs = lit_seq2seq_trained.generate(dm_seq2seq.dataset, start_seq=get_music_from_tokens(tokenizer, *tokenizer[position]))\n",
    "show_and_write(notes, durs, \"example_recreate.midi\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9f91a12e41d9525743075a4f29f964af19eccf88374fc768e771b849eb39165b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('music-gen')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
